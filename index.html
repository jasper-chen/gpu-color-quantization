<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>GPU Color Quantization</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">GPU Color Quantization</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/griffint/SofSys2015DebuggingDungbeetles/tree/master/kmeans_cpu" class="btn">View on GitHub</a>
      <a href="https://github.com/griffint/SofSys2015DebuggingDungbeetles/archive/master.zip" class="btn">Download .zip</a>
    </section>

    <section class="main-content">
      <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Project Goal</h3>

<p> Our goal was to learn more about parallel computing and compare the performance of a k-means clustering algorithm in serial code with one written with parallelizable code.</p>

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is CUDA?</h3>

<p>CUDA is a parallel computing platform created by NVIDIA. It lets you use your GPU to run general purpose programs in parallel, which can give massive speed boosts to well designed programs. CUDA only runs on Nvidia graphics cards. However, there are several open source alternatives to CUDA if you have a different brand of GPU.</p>

<center><img src="./cuda.jpg"/></center>

<h3>
<a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is K-means color quantization?</h3>

<p> K-means clustering is a simple algorithm for partitioning points into clusters. Each cluster has a centroid, which is the mean of all points in that cluster. The algorithm assigns all points to the nearest centroid, then recalculates the coordinates of that centroid as the mean of all points in the cluster. It then repeats this process for a specified number of rounds or until no points shift clusters during a round.
Color quantization is a process for reducing the number of distinct colors in an image. This can greatly help save memory space to store an image. To store each pixel as full RGB data,  we require 24 bits of data. However, if we only have 256 different colors we can index them and each pixel only requires 8 bits of data. This can reduce the size of image files by up to 66% while keeping the image at the same resolution. K-means assigns pixels to a cluster in RGB space, then replaces that pixel with the RGB value of that centroid. </p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Serial Code Implementation</h3>

<p>First we implemented our algorithm in serial CPU code. This was fairly simple and ran predictably slow with very large images. Below is a diagram showing the steps of k-means clustering. </p>

<img src="./kmeans.png"/>

<h3>
<a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span class="octicon octicon-link"></span></a>Parallel Implementation</h3>

<p>Next we needed to implement this same algorithm using CUDA. Luckily, we were able to reuse much of the same code, we just needed to figure out what to run in parallel. There were 2 parts of the algorithm we wanted to speed up by running in parallel. The first was the assignment of pixels to clusters. The distance between each centroid and each pixel must be calculated, so we ran this calculation for each pixel in parallel. In serial code, the speed of this operation is roughly O(c*r*y) where c is the number of columns, r is the number of rows, and y is the number of clusters. Once this is run in parallel, the time complexity becomes O(r*y) because each row of pixels is run in parallel. This means that for a 1000x1000 image this step of the algorithm is sped up 1000 times. 

The next operation to run in parallel was remapping of the original image pixels to the new cluster colors. The time complexity for this operation is just O(p) where p is the number of pixels. We run sets of 3 pixels in parallel, such that time complexity is O(p/3). 

All of these operations do have a set amount of time needed to instantiate the CUDA kernel. However, we believe this time will be negligible compared to the potential speed benefits of running these parts of the algorithm in parallel.</p>

<h3>
<a id="performance" class="anchor" href="#performance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Performance</h3>

<p>Next we needed to implement this same algorithm using CUDA. Luckily, we were able to reuse much of the same code, we just needed to figure out what to run in parallel. There were 2 parts of the algorithm we wanted to speed up by running in parallel. The first was the assignment of pixels to clusters. The distance between each centroid and each pixel must be calculated, so we ran this calculation for each pixel in parallel. In serial code, the speed of this operation is roughly O(c*r*y) where c is the number of columns, r is the number of rows, and y is the number of clusters. Once this is run in parallel, the time complexity becomes O(r*y) because each row of pixels is run in parallel. This means that for a 1000x1000 image this step of the algorithm is sped up 1000 times. 

The next operation to run in parallel was remapping of the original image pixels to the new cluster colors. The time complexity for this operation is just O(p) where p is the number of pixels. We run sets of 3 pixels in parallel, such that time complexity is O(p/3). 

All of these operations do have a set amount of time needed to instantiate the CUDA kernel. However, we believe this time will be negligible compared to the potential speed benefits of running these parts of the algorithm in parallel.</p>

<img src="./performance.png"/>

<h3>
<a id="examples" class="anchor" href="#examples" aria-hidden="true"><span class="octicon octicon-link"></span></a>Examples</h3>

<p>The number at the top left indicates the number of centroids (colors used). </p>

<center><img src="./nba.gif"/><br></center>
<center><img src="./candidates.gif"/><br></center>
<center><img src="./natalie.gif"/><br></center>

<h3>
<a id="contributors" class="anchor" href="#contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Contributors</h3>

<p>Griffin Tschurwald, Jasper Chen, Jack Fan

</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/jasper-chen/gpu-color-quantization">GPU Color Quantization</a> is maintained by <a href="https://github.com/jasper-chen">Jasper Chen</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
